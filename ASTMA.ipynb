{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b850eafa",
   "metadata": {},
   "source": [
    "# Exp 1 - Sentiment Analysis by a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af0cf0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\sidsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdce36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading and wragling data\n",
    "\n",
    "df_avatar = pd.read_csv('IMDB Dataset.csv', engine='python')\n",
    "\n",
    "df_avatar_lines = df_avatar.groupby('sentiment').count()\n",
    "\n",
    "df_avatar_lines = df_avatar_lines.sort_values(by=['review'], ascending=False)[:10]\n",
    "\n",
    "top_character_names = df_avatar_lines.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cec7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out non-top characters\n",
    "\n",
    "df_character_sentiment = df_avatar[df_avatar['sentiment'].isin(top_character_names)]\n",
    "\n",
    "df_character_sentiment = df_character_sentiment[['sentiment', 'review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2222d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating sentiment score\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_character_sentiment.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_character_sentiment[['neg', 'neu', 'pos', 'compound']] = df_character_sentiment['review'].apply(sid.polarity_scores).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eaec73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.9951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.9641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.9605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.9213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.9744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.9890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>negative</td>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.6693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>negative</td>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.9851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>negative</td>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.7648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>negative</td>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                             review    neg  \\\n",
       "0      positive  One of the other reviewers has mentioned that ...  0.203   \n",
       "1      positive  A wonderful little production. <br /><br />The...  0.053   \n",
       "2      positive  I thought this was a wonderful way to spend ti...  0.094   \n",
       "3      negative  Basically there's a family where a little boy ...  0.138   \n",
       "4      positive  Petter Mattei's \"Love in the Time of Money\" is...  0.052   \n",
       "...         ...                                                ...    ...   \n",
       "49995  positive  I thought this movie did a down right good job...  0.047   \n",
       "49996  negative  Bad plot, bad dialogue, bad acting, idiotic di...  0.166   \n",
       "49997  negative  I am a Catholic taught in parochial elementary...  0.208   \n",
       "49998  negative  I'm going to have to disagree with the previou...  0.105   \n",
       "49999  negative  No one expects the Star Trek movies to be high...  0.135   \n",
       "\n",
       "         neu    pos  compound  \n",
       "0      0.748  0.048   -0.9951  \n",
       "1      0.776  0.172    0.9641  \n",
       "2      0.714  0.192    0.9605  \n",
       "3      0.797  0.065   -0.9213  \n",
       "4      0.801  0.147    0.9744  \n",
       "...      ...    ...       ...  \n",
       "49995  0.753  0.199    0.9890  \n",
       "49996  0.720  0.114   -0.6693  \n",
       "49997  0.683  0.108   -0.9851  \n",
       "49998  0.813  0.082   -0.7648  \n",
       "49999  0.723  0.141    0.4329  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_character_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd7414",
   "metadata": {},
   "source": [
    "# Exp 2 - Named Identity Recognition using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa9714bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for ent in doc.ents:\n",
    "\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07539554",
   "metadata": {},
   "source": [
    "# Exp 3 - Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e8215",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d965cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers  :  programm\n",
      "program  :  program\n",
      "with  :  with\n",
      "programming  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "sentence = \"Programmers program with programming languages\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for w in words:\n",
    "\tprint(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e42227",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c4bb484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sidsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites ---> kite\n",
      "babies ---> baby\n",
      "dogs ---> dog\n",
      "flying ---> flying\n",
      "smiling ---> smiling\n",
      "driving ---> driving\n",
      "died ---> died\n",
      "tried ---> tried\n",
      "feet ---> foot\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n",
    "\t\t'driving', 'died', 'tried', 'feet']\n",
    "for words in list1:\n",
    "\tprint(words + \" ---> \" + wnl.lemmatize(words))\n",
    "\t\n",
    "#> kites ---> kite\n",
    "#> babies ---> baby\n",
    "#> dogs ---> dog\n",
    "#> flying ---> flying\n",
    "#> smiling ---> smiling\n",
    "#> driving ---> driving\n",
    "#> died ---> died\n",
    "#> tried ---> tried\n",
    "#> feet ---> foot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cba65",
   "metadata": {},
   "source": [
    "# Exp 4 - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21a87fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eating</th>\n",
       "      <th>going</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>netflix</th>\n",
       "      <th>suits</th>\n",
       "      <th>watching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>review1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         eating  going  hate  love  netflix  suits  watching\n",
       "review1       0      0     0     2        1      1         2\n",
       "review2       1      1     2     0        0      0         0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"I love watching Netflix. I love watching Suits\", \"I hate going out. I hate eating out\"]\n",
    "df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_matrix = cv.fit_transform(df['text'])\n",
    "df_dtm = pd.DataFrame(cv_matrix.toarray(),\n",
    " index=df['review'].values,\n",
    " columns=cv.get_feature_names())\n",
    "df_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e322fc7",
   "metadata": {},
   "source": [
    "# Exp 5 - Term Frequency–Inverse Document Frequency(TF - IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "917ef1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "880c08ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>hate</th>\n",
       "      <th>java</th>\n",
       "      <th>love</th>\n",
       "      <th>python</th>\n",
       "      <th>writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>review1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.81093</td>\n",
       "      <td>2.81093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.81093</td>\n",
       "      <td>2.81093</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         code     hate     java     love   python  writing\n",
       "review1   2.0  0.00000  0.00000  2.81093  2.81093      1.0\n",
       "review2   2.0  2.81093  2.81093  0.00000  0.00000      1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"I love writing code in Python. I love Python code\",\n",
    " \"I hate writing code in Java. I hate Java code\"]\n",
    "df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})\n",
    "tfidf = TfidfVectorizer(stop_words='english', norm=None)\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "df_dtm = pd.DataFrame(tfidf_matrix.toarray(),\n",
    " index=df['review'].values,\n",
    " columns=tfidf.get_feature_names())\n",
    "df_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f1ebd",
   "metadata": {},
   "source": [
    "# Exp 6 - Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cb99f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sidsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1d294a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n",
      "first met quiet. remained quiet entire two hour long journey Stony Brook New York.\n",
      "Old length:  129\n",
      "New length:  82\n"
     ]
    }
   ],
   "source": [
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)\n",
    "print(len(sw_nltk))\n",
    "text = \"When I first met her she was very quiet. She remained quiet during the entire two hour long journey from Stony Brook to New York.\"\n",
    "words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)\n",
    "print(\"Old length: \", len(text))\n",
    "print(\"New length: \", len(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03380f8d",
   "metadata": {},
   "source": [
    "# Exp 7 - POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5e21212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sidsr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be66e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
      "[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
      "[('Marriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('’', 'NN'), ('life.It', 'NN'), ('exciting', 'VBG'), ('frightening', 'NN'), ('.', '.')]\n",
      "[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people.It', 'NN'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never', 'RB'), ('found', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n",
    " \"Sukanya is getting married next year. \" \\\n",
    " \"Marriage is a big step in one’s life.\" \\\n",
    " \"It is both exciting and frightening. \" \\\n",
    " \"But friendship is a sacred bond between people.\" \\\n",
    " \"It is a special kind of love between us. \" \\\n",
    " \"Many of you must have tried searching for a friend \"\\\n",
    " \"but never found the right one.\"\n",
    " \n",
    "# sent_tokenize is one of instances of\n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
    "\n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "    # Word tokenizers is used to find the words\n",
    "    # and punctuation in a string\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    # Using a Tagger. Which is part-of-speech\n",
    "    # tagger or POS-tagger.\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    print(tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93134d73",
   "metadata": {},
   "source": [
    "# Exp 8 - Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5417b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP the/DT book/NN) has/VBZ (NP many/JJ chapters/NNS))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = [\n",
    " (\"the\", \"DT\"),\n",
    " (\"book\", \"NN\"),\n",
    " (\"has\",\"VBZ\"),\n",
    " (\"many\",\"JJ\"),\n",
    " (\"chapters\",\"NNS\")\n",
    "]\n",
    "chunker = nltk.RegexpParser(\n",
    " r'''\n",
    " NP:{<DT><NN.*><.*>*<NN.*>}\n",
    " }<VB.*>{\n",
    " '''\n",
    ")\n",
    "chunker.parse(sentence)\n",
    "Output = chunker.parse(sentence)\n",
    "print(Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237de7f1",
   "metadata": {},
   "source": [
    "# Exp 9 - WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17bfa5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "587f29a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'malign', 'vicious', 'evilness', 'malefic', 'immorality', 'wickedness', 'malevolent', 'evil', 'iniquity'}\n",
      "{'goodness', 'good'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "for synset in wordnet.synsets(\"evil\"):\n",
    "    for l in synset.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec6027",
   "metadata": {},
   "source": [
    "# Exp 10 - Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fba50f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.2-cp38-cp38-win_amd64.whl (153 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\sidsr\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae156af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3ce1aad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preprocessed Data:  datasets used machine-learning research cited peer reviewed academic journals datasets integral part field machine learning major advances field result advances learning algorithms ( deep learning ) , computer hardware , , less-intuitively , availability high-quality training datasets [ 1 ] high quality labeled training datasets supervised semi-supervised machine learning algorithms usually difficult expensive produce large amount time needed label data although need labeled , high-quality datasets unsupervised learning also difficult costly produce\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot open resource",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f0c6e375dd2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mclean_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordcloud_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mwordcloud_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_word_cloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-f0c6e375dd2f>\u001b[0m in \u001b[0;36mcreate_word_cloud\u001b[1;34m(self, final_data)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"red\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# call the generate method of WordCloud class to generate an image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sans-serif\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"black\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m# plt the image generated by WordCloud class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \"\"\"\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m                 \u001b[1;31m# try to find a position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m                 \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m                 \u001b[1;31m# transpose font optionally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 transposed_font = ImageFont.TransposedFont(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfreetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36mfreetype\u001b[1;34m(font)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfreetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFreeTypeFont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    207\u001b[0m                         \u001b[0mload_from_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             self.font = core.getfont(\n\u001b[0m\u001b[0;32m    210\u001b[0m                 \u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             )\n",
      "\u001b[1;31mOSError\u001b[0m: cannot open resource"
     ]
    }
   ],
   "source": [
    "class WordCloudGeneration:\n",
    "    def preprocessing(self, data):\n",
    "        # convert all words to lowercase\n",
    "        data = [item.lower() for item in data]\n",
    "        # load the stop_words of english\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        # concatenate all the data with spaces.\n",
    "        paragraph = ' '.join(data)\n",
    "        # tokenize the paragraph using the inbuilt tokenizer\n",
    "        word_tokens = word_tokenize(paragraph) \n",
    "        # filter words present in stopwords list \n",
    "        preprocessed_data = ' '.join([word for word in word_tokens if not word in stop_words])\n",
    "        print(\"\\n Preprocessed Data: \" ,preprocessed_data)\n",
    "        return preprocessed_data\n",
    "    def create_word_cloud(self, final_data):\n",
    "        # initiate WordCloud object with parameters width, height, maximum font size and background \n",
    "        color = \"red\"\n",
    "        # call the generate method of WordCloud class to generate an image\n",
    "        wordcloud = WordCloud(font_path=\"sans-serif\", width=1600, height=800, max_font_size=200, background_color=\"black\").generate(final_data)   \n",
    "        # plt the image generated by WordCloud class\n",
    "        plt.figure(figsize=(12,10))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "wordcloud_generator = WordCloudGeneration()\n",
    "# you may uncomment the following line to use custom input\n",
    "# input_text = input(\"Enter the text here: \")\n",
    "input_text = 'These datasets are used for machine-learning research and have been cited in peer reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets.[1] High quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.'\n",
    "input_text = input_text.split('.')\n",
    "\n",
    "clean_data = wordcloud_generator.preprocessing(input_text)\n",
    "\n",
    "wordcloud_generator.create_word_cloud(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44323fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
